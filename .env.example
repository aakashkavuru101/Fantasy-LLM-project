# Environment Configuration
ENVIRONMENT=development

# Model Configuration
BASE_MODEL=meta-llama/Llama-2-7b-chat-hf
ADAPTER_DIR=checkpoints/fantasy-lora
MAX_NEW_TOKENS=128
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50

# Data Configuration  
TOKEN_DIR=data/tokenized
DATASET_LIMIT=20000
MAX_LENGTH=512

# Training Configuration
OUTPUT_DIR=checkpoints/fantasy-lora
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
NUM_TRAIN_EPOCHS=3
LEARNING_RATE=2e-4
LORA_R=64
LORA_ALPHA=128
LORA_DROPOUT=0.05

# Weights & Biases
WANDB_PROJECT=fantasy-llm
WANDB_ENTITY=your-wandb-entity
WANDB_API_KEY=your-wandb-api-key
# Set to 'offline' to disable wandb logging
WANDB_MODE=online

# Gradio Configuration
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
API_RELOAD=true
API_ACCESS_LOG=true

# Security
SECRET_KEY=your-secret-key-here-change-this
API_KEY=your-api-key-here

# Database (if using)
DATABASE_URL=sqlite:///./fantasy_llm.db

# Redis (if using for caching)
REDIS_URL=redis://localhost:6379/0

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# CUDA/GPU Configuration
CUDA_VISIBLE_DEVICES=0
TORCH_HOME=./models/torch_cache

# HuggingFace Configuration
HF_HOME=./models/huggingface_cache
HF_TOKEN=your-huggingface-token